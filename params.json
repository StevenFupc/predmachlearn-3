{"name":"Predmachlearn","tagline":"","body":"Activity Quality Prediction\r\n========================================================\r\n## Introduction\r\nNowadays it is possible to collect a large amount of data about personal activity using devices such as Jawbone Up, Nike FuelBand, and Fitbit. These devices are popular among enthusiasts like sportsmen, scientists and of cause tech geeks. In this project we will use data collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict activity quality. \r\n\r\n## Methods\r\n### Data Collection\r\nAccording to [this paper](http://groupware.les.inf.puc-rio.br/har) six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). The data were recorded from accelerometers. We downloaded [train](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) datasets on the 2nd of June, 2014.\r\n\r\n```r\r\nset.seed(1111)\r\nlibrary(caret, quietly = T)\r\nsetwd(\"~/Doc/Coursera/PractML/proj\")\r\ndata <- read.csv(\"train.csv\", na.strings = \"NA\", header = T)\r\ndata <- data[sample(nrow(data)), ]\r\ninTrain <- createDataPartition(data$X, p = 0.7)[[1]]\r\ntrain <- data[inTrain, ]\r\nvalid <- data[-inTrain, ]\r\ntest <- read.csv(\"test.csv\", na.strings = \"NA\", header = T)\r\n```\r\n\r\n\r\n### Exploratory Analysis\r\nExploratory analysis was performed by examining tables of the observed data. Having observed them, we identified transformations to perform on the raw data. Exploratory analysis was used to\r\n\r\n1.  Verify the quality of the data\r\n2.  Identify missing values\r\n3.  Determine the terms to be used in the predictive model. \r\n\r\nWe found that the train set contained 1287472 missing values, the test set had 2000 missing values. As the number of NA's in columns was relativly big we deleted those columns. Also we deleted the columns with service information such as \"X\", \"cvtd_timestamp\", etc.\r\n\r\n```r\r\nidx2remove <- function(df) {\r\n    idx <- c()\r\n    for (i in 1:dim(df)[2]) {\r\n        if (sum(is.na(df[, i]))/dim(df)[1] > 0.1) \r\n            idx <- c(idx, i)\r\n    }\r\n    return(idx)\r\n}\r\ncol.to.remove <- unique(c(idx2remove(train), idx2remove(valid), idx2remove(test)))\r\n\r\ntrainC <- train$classe\r\ntrain <- train[, -c(col.to.remove)]\r\n\r\nvalidC <- valid$classe\r\nvalid <- valid[, -c(col.to.remove)]\r\n\r\ntestC <- test$problem_id\r\ntest <- test[, -c(col.to.remove)]\r\n\r\ntrain <- train[, sapply(train, is.numeric)]\r\nvalid <- valid[, sapply(valid, is.numeric)]\r\ntest <- test[, sapply(test, is.numeric)]\r\n```\r\n\r\n\r\n### Confounders\r\nWe identified variables with correlation over 0.9. This step allowed us to remove 3 additional columns from the each data set. \r\n\r\n```r\r\nnum.idx <- which(sapply(train, class) == \"numeric\")\r\ndescrCorr <- abs(cor(train[, num.idx]))\r\nhighCorr <- findCorrelation(descrCorr, 0.9)\r\nhighCorrCol <- dimnames(descrCorr[, highCorr])[[2]]\r\ntrain <- train[, !names(train) %in% c(highCorrCol, \"X\", \"cvtd_timestamp\", \"raw_timestamp_part_1\", \r\n    \"raw_timestamp_part_2\", \"num_window\", \"classe\")]\r\nvalid <- valid[, !names(valid) %in% c(highCorrCol, \"X\", \"cvtd_timestamp\", \"raw_timestamp_part_1\", \r\n    \"raw_timestamp_part_2\", \"num_window\", \"classe\")]\r\ntest <- test[, !names(test) %in% c(highCorrCol, \"X\", \"cvtd_timestamp\", \"raw_timestamp_part_1\", \r\n    \"raw_timestamp_part_2\", \"num_window\", \"problem_id\")]\r\ntrain <- sapply(train, as.numeric)\r\nvalid <- sapply(valid, as.numeric)\r\ntest <- as.matrix(sapply(test, as.numeric))\r\n```\r\n\r\n\r\n### Statistical Modeling\r\nA simple linear or logistic regression is not a proper choice for building a predictive model with high-dimensional input data. We chose Random Forest from the __randomForest__ package. [“It runs efficiently on large data bases and can handle thousands of input variables without variable deletion. It gives estimates of what variables are important in the classification. It has methods for balancing error in class population unbalanced data sets”][rf1]. As stated by the\r\nauthors, the Random forest does not overfit. We built the model for the outcome variable *classe* using 49 predictive variables. \r\n\r\n```r\r\nlibrary(randomForest, quietly = T)\r\n```\r\n\r\n```\r\n## randomForest 4.6-7\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n```\r\n\r\n```r\r\nfitRF <- randomForest(x = train, y = trainC, importance = T, ntree = 317)\r\n```\r\n\r\nHere we can see 5 the most important variables\r\n\r\n```r\r\nvarImpPlot(fitRF, n.var = 5, type = 1, main = \"Fig. 1. Variable Importance\")\r\n```\r\n![](https://github.com/kailex/predmachlearn/blob/master/figure/unnamed-chunk-5.png)\r\n\r\nWith number of trees of 317 the Random Forest was successfully trained and the estimated OOB error rate (in sample error) for the training set was 0.52%. We don't need to do cross-validation as random forest uses bootstrap samples from the train set.\r\n\r\n### Results\r\nOn the fig. 1 we can see the most important variables for the predicting model. \r\n\r\nVariable  | Mean Decrease in Accuracy\r\n----------| -------------\r\nyaw_belt          | 51.84860\r\nroll_belt         | 45.92186\r\npitch_belt        | 43.11729\r\n\r\nLet's plot these three variables. On the figures 2-4 we can see the relationship between those variables. We color grouped the points based on the 5 levels of activities quality. The figures show rather complicated patterns which cannot be linearly separated, thus, we used random forest.\r\n\r\n\r\n```r\r\npar(mfrow = c(2, 2))\r\nvalid <- data.frame(valid)\r\nplot(valid$yaw_belt, valid$roll_belt, xlab = \"Yaw belt\", ylab = \"Roll belt\", \r\n    main = \"Fig. 2\", col = as.numeric(validC), pch = 19, cex = 0.5)\r\n\r\nplot(valid$yaw_belt, valid$pitch_belt, xlab = \"Yaw belt\", ylab = \"Pitch belt\", \r\n    main = \"Fig. 3\", col = as.numeric(validC), pch = 19, cex = 0.5)\r\n\r\nplot(valid$roll_belt, valid$pitch_belt, xlab = \"Roll belt\", ylab = \"Pitch belt\", \r\n    main = \"Fig. 4\", col = as.numeric(validC), pch = 19, cex = 0.5)\r\nplot.new()\r\nlegend(\"center\", legend = levels(validC), cex = 2, text.col = seq_along(levels(validC)))\r\n```\r\n\r\n![](https://github.com/kailex/predmachlearn/blob/master/figure/unnamed-chunk-6.png) \r\n\r\nWe tested out model on validation set and observed a confusion matrix for the validation set.\r\n\r\n```r\r\npredRF <- predict(fitRF, valid)\r\nconfusionMatrix(predRF, validC)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1663   11    0    0    0\r\n##          B    0 1127   10    0    0\r\n##          C    0    2 1019   13    0\r\n##          D    0    0    1  967    0\r\n##          E    0    0    0    0 1071\r\n## \r\n## Overall Statistics\r\n##                                         \r\n##                Accuracy : 0.994         \r\n##                  95% CI : (0.991, 0.996)\r\n##     No Information Rate : 0.283         \r\n##     P-Value [Acc > NIR] : <2e-16        \r\n##                                         \r\n##                   Kappa : 0.992         \r\n##  Mcnemar's Test P-Value : NA            \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             1.000    0.989    0.989    0.987    1.000\r\n## Specificity             0.997    0.998    0.997    1.000    1.000\r\n## Pos Pred Value          0.993    0.991    0.985    0.999    1.000\r\n## Neg Pred Value          1.000    0.997    0.998    0.997    1.000\r\n## Prevalence              0.283    0.194    0.175    0.167    0.182\r\n## Detection Rate          0.283    0.192    0.173    0.164    0.182\r\n## Detection Prevalence    0.285    0.193    0.176    0.165    0.182\r\n## Balanced Accuracy       0.999    0.993    0.993    0.993    1.000\r\n```\r\n\r\nOut of sample error is about 0.63%. Finally we tested our model on the 20 test cases available in the test data. We got 100% prediction accuracy.\r\n\r\n```r\r\npredRFT <- predict(fitRF, test)\r\nanswer <- as.character(predRFT)\r\npml_write_files <- function(x) {\r\n    n <- length(x)\r\n    for (i in 1:n) {\r\n        filename <- paste0(\"problem_id_\", i, \".txt\")\r\n        write.table(x[i], file <- filename, quote = FALSE, row.names = FALSE, \r\n            col.names = FALSE)\r\n    }\r\n}\r\npml_write_files(answer)\r\n```\r\n\r\n\r\n### Conclusions\r\nIn this paper we proposed a predictive model based on Random Forest. This model provides high accuracy in identifying of activity quality. Also it reveals the most important variables for prediction.\r\nSome potential problems should be noted. As it's stated by the author, Random Forest does not need “cross-validation or a separate test set to get an unbiased estimate of the test set error” but that's true only for independent and identically-distributed data. Also we should mention that the measure of importance of variables can be biased. Thus, variable importance can be overestimated.\r\n\r\n[rf1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm \"Random Forests\"\"\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}